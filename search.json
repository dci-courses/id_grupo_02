[
  {
    "objectID": "03.- API Extractor/reddit2.html",
    "href": "03.- API Extractor/reddit2.html",
    "title": "id_grupo_02",
    "section": "",
    "text": "import praw\nimport pandas as pd\nimport datetime\n\n# Conectar con Reddit usando PRAW\nreddit = praw.Reddit(\"CREDENCIALES\")\n\n# Extraer datos de un subreddit\nsubreddit = reddit.subreddit('lies')\nposts = []\nfor post in subreddit.new(limit=500000):  # Extraer top 5000 publicaciones\n    posts.append([post.title, post.score, post.id, post.subreddit, post.num_comments, \n                  post.selftext, post.created, post.author, post.upvote_ratio])\n\n# Convertir los datos a un DataFrame\ndf = pd.DataFrame(posts, columns=['title', 'score', 'id', 'subreddit', 'num_comments', \n                                  'body', 'created', 'author', 'upvote_ratio'])\n\n# Convertir la columna 'created' a formato de fecha\ndf['created'] = pd.to_datetime(df['created'], unit='s')\n\n# Mostrar las primeras filas\ndf.head()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[22], line 34\n     31     posts_per_author[author].append(post)\n     33 # Sorting the dictionary by author name\n---&gt; 34 sorted_posts_per_author = dict(sorted(posts_per_author.items(), key=lambda item: item[0].name))\n     36 # Example of converting to DataFrame\n     37 data = []\n\nCell In[22], line 34, in &lt;lambda&gt;(item)\n     31     posts_per_author[author].append(post)\n     33 # Sorting the dictionary by author name\n---&gt; 34 sorted_posts_per_author = dict(sorted(posts_per_author.items(), key=lambda item: item[0].name))\n     36 # Example of converting to DataFrame\n     37 data = []\n\nAttributeError: 'NoneType' object has no attribute 'name'\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Contar el número de publicaciones por fecha\ndf['date'] = df['created'].dt.date\nposts_per_day = df.groupby('date').size()\n\n# Graficar la cantidad de publicaciones a lo largo del tiempo\nplt.figure(figsize=(10,6))\nposts_per_day.plot()\nplt.title('Publicaciones en el subreddit Python a lo largo del tiempo')\nplt.xlabel('Fecha')\nplt.ylabel('Número de publicaciones')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Graficar la distribución de puntajes\nplt.figure(figsize=(10,6))\ndf['score'].plot(kind='hist', bins=50)\nplt.title('Distribución de puntajes de las publicaciones')\nplt.xlabel('Puntaje (upvotes)')\nplt.ylabel('Frecuencia')\nplt.show()",
    "crumbs": [
      "03.- API Extractor",
      "reddit2.html"
    ]
  },
  {
    "objectID": "index.html#grupo-dos",
    "href": "index.html#grupo-dos",
    "title": "01.- Presentacion proyecto",
    "section": "Grupo dos",
    "text": "Grupo dos\n\nCamila Infante\nCarlos Peña",
    "crumbs": [
      "01.- Presentacion proyecto"
    ]
  },
  {
    "objectID": "02.- Colored eyes/colored_eyes.html",
    "href": "02.- Colored eyes/colored_eyes.html",
    "title": "Colored Eyes",
    "section": "",
    "text": "Dejar el objeto frente a la camara por 5 segundos.",
    "crumbs": [
      "02.- Colored eyes",
      "Colored Eyes"
    ]
  },
  {
    "objectID": "04.- MP3/mp3.html",
    "href": "04.- MP3/mp3.html",
    "title": "id_grupo_02",
    "section": "",
    "text": "Primero se debe instalar todas las librerías necesarias para leer y analizar los datos.\n\n\n# !pip install numpy\n# !pip install pandas\n# !pip install scikit-learn\n# !pip install seaborn\n# !pip install spacy\n# !pip install nltk\n# !pip install matplotlib\n# ! python -m spacy download es_core_news_sm\n\nLuego, importar las librerias instaladas.\n\nimport pandas as pd    \nimport spacy\nimport nltk \nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer  \nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, f1_score, classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\n\nfrom spacy.lang.es.stop_words import STOP_WORDS\nnlp = spacy.load(\"es_core_news_sm\", disable=['ner', 'parser', 'tagger'])\n\nCargar los datos y realizar un respaldo en caso de pérdida.\n\ndataset = pd.read_csv('datos.csv')\ndataset_r = dataset.copy(deep=True) # respaldo\n\nprint('Cargando datos...')\n\ndataset\n\nCargando datos...\n\n\n\n\n\n\n\n\n\nproductAsin\ncountry\ndate\nisVerified\nratingScore\nreviewTitle\nreviewDescription\nreviewUrl\nreviewedIn\nvariant\nvariantAsin\n\n\n\n\n0\nB09G9BL5CP\nIndia\n11-08-2024\nTrue\n4\nNo charger\nEvery thing is good about iPhones, there's not...\nhttps://www.amazon.in/gp/customer-reviews/R345...\nReviewed in India on 11 August 2024\nColour: MidnightSize: 256 GB\nB09G9BQS98\n\n\n1\nB09G9BL5CP\nIndia\n16-08-2024\nTrue\n5\niPhone 13 256GB\nIt look so fabulous, I am android user switche...\nhttps://www.amazon.in/gp/customer-reviews/R2HJ...\nReviewed in India on 16 August 2024\nColour: MidnightSize: 256 GB\nB09G9BQS98\n\n\n2\nB09G9BL5CP\nIndia\n14-05-2024\nTrue\n4\nFlip camera option nill\nI tried to flip camera while recording but no ...\nhttps://www.amazon.in/gp/customer-reviews/R3Y7...\nReviewed in India on 14 May 2024\nColour: MidnightSize: 256 GB\nB09G9BQS98\n\n\n3\nB09G9BL5CP\nIndia\n24-06-2024\nTrue\n5\nProduct\n100% genuine\nhttps://www.amazon.in/gp/customer-reviews/R1P9...\nReviewed in India on 24 June 2024\nColour: MidnightSize: 256 GB\nB09G9BQS98\n\n\n4\nB09G9BL5CP\nIndia\n18-05-2024\nTrue\n5\nGood product\nHappy to get the iPhone 13 in Amazon offer\nhttps://www.amazon.in/gp/customer-reviews/R1XI...\nReviewed in India on 18 May 2024\nColour: MidnightSize: 256 GB\nB09G9BQS98\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3057\nB09G9D8KRQ\nIndia\n11-10-2023\nTrue\n1\nVery bad experience with i phone 13\nUseless phon never buy this heat n useless cam...\nhttps://www.amazon.in/gp/customer-reviews/R10O...\nReviewed in India on 11 October 2023\nColour: (PRODUCT) REDSize: 128 GB\nB09G99CW2N\n\n\n3058\nB09G9D8KRQ\nIndia\n14-10-2022\nTrue\n2\nnot happy with this apple product\niam not happy with this product why because ch...\nhttps://www.amazon.in/gp/customer-reviews/R2FW...\nReviewed in India on 14 October 2022\nColour: (PRODUCT) REDSize: 128 GB\nB09G99CW2N\n\n\n3059\nB09G9D8KRQ\nIndia\n24-02-2022\nTrue\n3\nGood phone\nGood phone\nhttps://www.amazon.in/gp/customer-reviews/R2C7...\nReviewed in India on 24 February 2022\nColour: (PRODUCT) REDSize: 128 GB\nB09G99CW2N\n\n\n3060\nB09G9D8KRQ\nIndia\n16-10-2023\nTrue\n1\nBattery discharge\nWhile charging mobile it's getting so hot even...\nhttps://www.amazon.in/gp/customer-reviews/R3K0...\nReviewed in India on 16 October 2023\nColour: (PRODUCT) REDSize: 128 GB\nB09G99CW2N\n\n\n3061\nB09G9D8KRQ\nIndia\n11-11-2023\nTrue\n1\nBatter power needs to be improved\nBattery power is be very bad need to chat on d...\nhttps://www.amazon.in/gp/customer-reviews/R2QO...\nReviewed in India on 11 November 2023\nColour: (PRODUCT) REDSize: 128 GB\nB09G99CW2N\n\n\n\n\n3062 rows × 11 columns\n\n\n\nGraficar la distribución de puntajes.\n\nplt.figure(figsize=(10,6))\ndataset['ratingScore'].plot(kind='hist', bins=50)\nplt.title('Distribución de puntajes de las publicaciones')\nplt.xlabel('Puntaje (upvotes)')\nplt.ylabel('Frecuencia')\nplt.show()\n\n\n\n\n\n\n\n\nSe puede ver que existen una gran cantidad de respuestas positivas sobre el equipo. Sin embargo, también se puede apreciar una cantidad no menor de respuestas negativas al equipo.\n\ntamanos_subcategorias = dataset.groupby('ratingScore').size()\n\n# Crear un DataFrame con las subcategorías y sus tamaños\ndataset_subset = pd.DataFrame({\n    'Subcategoría': tamanos_subcategorias.index,\n    'Tamaño': tamanos_subcategorias.values\n})\n\n# Mostrar el DataFrame\ndataset_subset\n\n\n\n\n\n\n\n\nSubcategoría\nTamaño\n\n\n\n\n0\n1\n587\n\n\n1\n2\n171\n\n\n2\n3\n239\n\n\n3\n4\n461\n\n\n4\n5\n1604\n\n\n\n\n\n\n\n\nmin_samples = dataset_subset['Tamaño'].min()\n\nprint(f\"El número mínimo de ejemplos entre las subcategorías es: {min_samples}\")\n\ng = dataset.groupby('ratingScore')\n\ndataset_balanceado = g.apply(lambda x: x.sample(min_samples)).reset_index(drop=True)\n\n\nprint(f\"Tamaño del dataset balanceado: {len(dataset_balanceado)}\")\nprint(\"Tamaño por subcategoría después del balanceo:\")\nprint(dataset_balanceado.groupby('ratingScore').size())\n\ndataset_balanceado\n\nEl número mínimo de ejemplos entre las subcategorías es: 171\nTamaño del dataset balanceado: 855\nTamaño por subcategoría después del balanceo:\nratingScore\n1    171\n2    171\n3    171\n4    171\n5    171\ndtype: int64\n\n\n/tmp/ipykernel_72519/1001277432.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  dataset_balanceado = g.apply(lambda x: x.sample(min_samples)).reset_index(drop=True)\n\n\n\n\n\n\n\n\n\nproductAsin\ncountry\ndate\nisVerified\nratingScore\nreviewTitle\nreviewDescription\nreviewUrl\nreviewedIn\nvariant\nvariantAsin\n\n\n\n\n0\nB0BN72MLT2\nUnited States\n21-10-2023\nTrue\n1\nBuyers beware!\nThis phone came with a defect with the camera....\nhttps://www.amazon.com/gp/customer-reviews/R1C...\nReviewed in the United States on October 21, 2023\nService Provider: UnlockedColor: MidnightSize:...\nB0BN72MLT2\n\n\n1\nB09G9D8KRQ\nIndia\n17-10-2021\nFalse\n1\nApple is trying to loot people\nYou have to pay 95000+ for this stupid phone w...\nhttps://www.amazon.in/gp/customer-reviews/RABT...\nReviewed in India on 17 October 2021\nColour: BlueSize: 256 GB\nB09G93H3BR\n\n\n2\nB09G9D8KRQ\nIndia\n14-04-2024\nTrue\n1\nWorst Product. Extremely bad customer service\nPathetic experience... Unable to hear any soun...\nhttps://www.amazon.in/gp/customer-reviews/R1KN...\nReviewed in India on 14 April 2024\nColour: StarlightSize: 128 GB\nB09G9D8KRQ\n\n\n3\nB0BN72MLT2\nUnited States\n13-02-2024\nTrue\n1\nMobile defects\nI purchase this mobile on May 2022 and it is w...\nhttps://www.amazon.com/gp/customer-reviews/R26...\nReviewed in the United States on February 13, ...\nService Provider: UnlockedColor: BlueSize: 256GB\nB0BN71T1J7\n\n\n4\nB0BDK8LKPJ\nIndia\n31-03-2023\nTrue\n1\nHeating issues\nHeating issues .thought it will be ok in a day...\nhttps://www.amazon.in/gp/customer-reviews/R2N9...\nReviewed in India on 31 March 2023\nColour: StarlightSize: 128 GB\nB0BDK8LKPJ\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n850\nB09G9D8KRQ\nIndia\n10-06-2022\nTrue\n5\nWorth it\nNaN\nhttps://www.amazon.in/gp/customer-reviews/R2ZZ...\nReviewed in India on 10 June 2022\nColour: BlueSize: 256 GB\nB09G93H3BR\n\n\n851\nB0BN72MLT2\nUnited States\n17-05-2024\nTrue\n5\nIphone14\nI LOVE IT SM EXACTLY LIKS U THINK IT IS I reco...\nhttps://www.amazon.com/gp/customer-reviews/R2V...\nReviewed in the United States on May 17, 2024\nService Provider: UnlockedColor: PurpleSize: 1...\nB0BN71VW28\n\n\n852\nB0BN72MLT2\nUnited States\n09-10-2023\nTrue\n5\nMaravilloso!\nTodo excelente!!\nhttps://www.amazon.com/gp/customer-reviews/R2U...\nReviewed in the United States on October 9, 2023\nService Provider: UnlockedColor: BlueSize: 256GB\nB0BN71T1J7\n\n\n853\nB0BDK8LKPJ\nIndia\n18-10-2023\nTrue\n5\nAndroid to Apple Transition - Not a smooth exp...\nAlways feels good to hold this masterpiece. Th...\nhttps://www.amazon.in/gp/customer-reviews/R3GE...\nReviewed in India on 18 October 2023\nColour: BlueSize: 128 GB\nB0BDK62PDX\n\n\n854\nB0BN72MLT2\nUnited States\n27-07-2024\nTrue\n5\nFast Shipping, Product as Advertised\nPleasant Experience. Seller processed order q...\nhttps://www.amazon.com/gp/customer-reviews/R22...\nReviewed in the United States on July 27, 2024\nService Provider: UnlockedColor: YellowSize: 1...\nB0CG7ZDWGK\n\n\n\n\n855 rows × 11 columns\n\n\n\n\n# Graficar la distribución de puntajes\nplt.figure(figsize=(10,6))\ndataset_balanceado['ratingScore'].plot(kind='hist', bins=50)\nplt.title('Distribución de puntajes de las publicaciones')\nplt.xlabel('Puntaje (upvotes)')\nplt.ylabel('Frecuencia')\nplt.show()\n\n\n\n\n\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    dataset_balanceado.reviewDescription,\n    dataset_balanceado.ratingScore,\n    test_size=0.33,\n    random_state=42,\n)\n\n\nX_train\n\n781    very much satisfied with phone.... everything ...\n56     Don’t buy any one this is device is getting ve...\n291     Feature are good ,colour is not according to. Me\n316    Wifi issue after 6-7 months, wifi remains conn...\n842                                    Best in the range\n                             ...                        \n71              touch is very poor, not working properly\n106    This phone is locked!!  I can not use it on an...\n270    Everything about this iphone is good axcept ba...\n435    The phone is actually pretty good but it had s...\n102    When I opened the box 1 very small dent was ob...\nName: reviewDescription, Length: 572, dtype: object\n\n\n\nX_test\n\n66     No me deja poner el iCloud en el celular, dice...\n434    The phone heats a lot while using even for bas...\n198                                                 Good\n212    There was a very noticeable scratch on the sid...\n793    Great phone , loved it and the camera quality ...\n                             ...                        \n670    Except some few scratches, it is a very good m...\n815                   NYC looking  amazing  good service\n41     No biometrics with many UI flaws. Too expensiv...\n528    Design:The iPhone 13 retains the classic Apple...\n108                                           Very worst\nName: reviewDescription, Length: 283, dtype: object\n\n\n\n# Definimos el vectorizador para convertir el texto a BoW:\nvectorizer = CountVectorizer()  \n\n# Definimos el clasificador que usaremos.\nclf = MultinomialNB()   \n\n# Creamos el pipeline\ntext_clf = Pipeline([('vect', vectorizer), ('clf', clf)])\n\n\n# Fill NaN values in the reviewDescription column\nX_train = X_train.fillna('')\n\n# Fit the model\ntext_clf.fit(X_train, y_train)\n\nPipeline(steps=[('vect', CountVectorizer()), ('clf', MultinomialNB())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('vect', CountVectorizer()), ('clf', MultinomialNB())])  CountVectorizer?Documentation for CountVectorizerCountVectorizer()  MultinomialNB?Documentation for MultinomialNBMultinomialNB() \n\n\n\nX_test = X_test.fillna('')\n\ny_pred = text_clf.predict(X_test)\ny_pred\n\narray([1, 2, 5, 3, 5, 3, 5, 3, 1, 5, 4, 3, 1, 3, 1, 4, 3, 1, 4, 4, 2, 3,\n       4, 5, 4, 5, 5, 5, 4, 4, 5, 3, 1, 4, 5, 3, 1, 5, 1, 5, 1, 5, 3, 1,\n       3, 1, 3, 3, 3, 5, 3, 4, 1, 1, 3, 3, 2, 4, 1, 5, 1, 3, 3, 3, 5, 1,\n       1, 5, 3, 1, 1, 1, 2, 1, 5, 3, 3, 2, 1, 1, 5, 1, 2, 2, 3, 2, 3, 3,\n       2, 1, 2, 3, 3, 2, 2, 3, 5, 5, 5, 2, 3, 5, 5, 1, 2, 2, 2, 5, 1, 5,\n       3, 2, 3, 2, 3, 2, 4, 3, 1, 3, 3, 3, 2, 3, 5, 5, 2, 3, 1, 1, 3, 1,\n       1, 5, 2, 4, 2, 5, 1, 2, 4, 2, 2, 1, 2, 2, 1, 5, 5, 2, 1, 1, 4, 1,\n       2, 4, 3, 3, 3, 3, 5, 3, 5, 5, 1, 2, 1, 1, 1, 1, 5, 3, 1, 3, 5, 2,\n       4, 1, 2, 1, 4, 2, 4, 3, 5, 3, 2, 3, 5, 1, 2, 3, 5, 1, 1, 5, 2, 5,\n       3, 3, 5, 5, 3, 1, 3, 2, 1, 3, 5, 3, 5, 5, 1, 1, 1, 2, 2, 2, 1, 2,\n       2, 5, 4, 3, 1, 3, 5, 4, 4, 3, 3, 2, 3, 2, 2, 5, 2, 2, 5, 5, 1, 1,\n       1, 1, 3, 1, 3, 2, 3, 2, 5, 2, 3, 3, 1, 1, 1, 5, 2, 3, 2, 3, 1, 2,\n       1, 2, 3, 1, 5, 5, 5, 2, 1, 2, 4, 3, 4, 3, 3, 5, 1, 4, 1])\n\n\n\npd.DataFrame({'content': X_test, 'category':y_test, 'predicted category': y_pred})\n\n\n\n\n\n\n\n\ncontent\ncategory\npredicted category\n\n\n\n\n66\nNo me deja poner el iCloud en el celular, dice...\n1\n1\n\n\n434\nThe phone heats a lot while using even for bas...\n3\n2\n\n\n198\nGood\n2\n5\n\n\n212\nThere was a very noticeable scratch on the sid...\n2\n3\n\n\n793\nGreat phone , loved it and the camera quality ...\n5\n5\n\n\n...\n...\n...\n...\n\n\n670\nExcept some few scratches, it is a very good m...\n4\n3\n\n\n815\nNYC looking amazing good service\n5\n5\n\n\n41\nNo biometrics with many UI flaws. Too expensiv...\n1\n1\n\n\n528\nDesign:The iPhone 13 retains the classic Apple...\n4\n4\n\n\n108\nVery worst\n1\n1\n\n\n\n\n283 rows × 3 columns\n\n\n\n\n# usando la matriz de confusión:\n\n# eje x -&gt; predichos\n# eje y -&gt; clase real\n\nprint(confusion_matrix(y_test, y_pred))\n\n[[37  9  8  1  3]\n [16 20  9  3  7]\n [10 14 27  5  7]\n [ 4  9 18 11 16]\n [ 3  5 11  5 25]]\n\n\n\n# usando el classification report:\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           1       0.53      0.64      0.58        58\n           2       0.35      0.36      0.36        55\n           3       0.37      0.43      0.40        63\n           4       0.44      0.19      0.27        58\n           5       0.43      0.51      0.47        49\n\n    accuracy                           0.42       283\n   macro avg       0.42      0.43      0.41       283\nweighted avg       0.42      0.42      0.41       283\n\n\n\n\ntext_clf.predict([\n    (\"Super\")\n])\n\narray([4])\n\n\n\ntext_clf.predict([\n    (\"bad\")\n])\n\narray([2])\n\n\n\ntext_clf.predict([\n    (\"good but broke\")\n])\n\narray([2])\n\n\n\ntext_clf.predict([\n    (\"it is so so\")\n])\n\narray([3])\n\n\n\n# Tokenizers para CountVectorizer\n\n# Solo tokenizar el doc usando spacy.\ndef tokenizer(doc):\n    return [x.orth_ for x in nlp(doc)]\n\n\n# Tokenizar y remover las stopwords del doc\ndef tokenizer_with_stopwords(doc):\n    return [x.orth_ for x in nlp(doc) if x.orth_ not in STOP_WORDS]\n\n\n# Tokenizar y lematizar.\ndef tokenizer_with_lemmatization(doc):\n    return [x.lemma_ for x in nlp(doc)]\n\n# Tokenizar y hacer stemming.\ndef tokenizer_with_stemming(doc):\n    stemmer = SnowballStemmer('spanish')\n    return [stemmer.stem(word) for word in [x.orth_ for x in nlp(doc)]]\n\n\nTOKENIZER = tokenizer_with_stemming\n\n# Definimos el vectorizador para convertir el texto a BoW:\nvectorizer = CountVectorizer(analyzer='word',\n                             tokenizer=TOKENIZER,\n                             ngram_range=(1, 1))\n\n# Definimos el clasificador que usaremos.\nclf = MultinomialNB()   \n\n# Creamos el pipeline\ntext_clf_2 = Pipeline([('vect', vectorizer), ('clf', clf)])\n\n\ntext_clf_2.fit(X_train, y_train)\ny_pred = text_clf_2.predict(X_test)\n\n/home/carl/Proyectos/ing-datos/mp3/.entorno/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n\n\n\n# usando la matriz de confusión:\nprint(confusion_matrix(y_test, y_pred),\n      '\\n\\n-------------------------------------------------------\\n')\n# usando el classification report:\nprint(classification_report(y_test, y_pred))\n\n[[34  9  9  2  4]\n [14 21 11  2  7]\n [12 16 21  6  8]\n [ 3  9 18 13 15]\n [ 3  5 11  6 24]] \n\n-------------------------------------------------------\n\n              precision    recall  f1-score   support\n\n           1       0.52      0.59      0.55        58\n           2       0.35      0.38      0.37        55\n           3       0.30      0.33      0.32        63\n           4       0.45      0.22      0.30        58\n           5       0.41      0.49      0.45        49\n\n    accuracy                           0.40       283\n   macro avg       0.41      0.40      0.40       283\nweighted avg       0.40      0.40      0.39       283\n\n\n\n\n# Qué tokenizer usaremos?\nTOKENIZER = tokenizer_with_lemmatization\n\n# Definimos el vectorizador para convertir el texto a BoW:\nvectorizer = CountVectorizer(analyzer='word',\n                             tokenizer=TOKENIZER,\n                             ngram_range=(1, 1))\n\n# Ahora definimos regresión logística como clasificador.\nlog_mod = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000)   \nlog_pipe = Pipeline([('vect', vectorizer), ('clf', log_mod)])\n\n\nlog_pipe.fit(X_train, y_train)\ny_pred = log_pipe.predict(X_test)\n\n/home/carl/Proyectos/ing-datos/mp3/.entorno/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  warnings.warn(\n/home/carl/Proyectos/ing-datos/mp3/.entorno/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\n\n\n# usando la matriz de confusión:\nprint(confusion_matrix(y_test, y_pred),\n      '\\n\\n-------------------------------------------------------\\n')\n# usando el classification report:\nprint(classification_report(y_test, y_pred))\n\n[[23  8  9 15  3]\n [ 4 20 11  9 11]\n [11 12 15 17  8]\n [ 2  5 10 28 13]\n [ 3  6  3 11 26]] \n\n-------------------------------------------------------\n\n              precision    recall  f1-score   support\n\n           1       0.53      0.40      0.46        58\n           2       0.39      0.36      0.38        55\n           3       0.31      0.24      0.27        63\n           4       0.35      0.48      0.41        58\n           5       0.43      0.53      0.47        49\n\n    accuracy                           0.40       283\n   macro avg       0.40      0.40      0.40       283\nweighted avg       0.40      0.40      0.39       283\n\n\n\n\n# Qué tokenizer usaremos?\nTOKENIZER = tokenizer_with_lemmatization\n\n# Definimos el vectorizador para convertir el texto a BoW:\nvectorizer = CountVectorizer(analyzer='word',\n                             tokenizer=TOKENIZER,\n                             ngram_range=(1, 3))\n\n# Ahora definimos regresión logística como clasificador.\nlog_mod = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter = 1000)   \nlog_pipe = Pipeline([('vect', vectorizer), ('clf', log_mod)])\n\n\n# usando la matriz de confusión:\nprint(confusion_matrix(y_test, y_pred),\n      '\\n\\n-------------------------------------------------------\\n')\n# usando el classification report:\nprint(classification_report(y_test, y_pred))\n\n[[23  8  9 15  3]\n [ 4 20 11  9 11]\n [11 12 15 17  8]\n [ 2  5 10 28 13]\n [ 3  6  3 11 26]] \n\n-------------------------------------------------------\n\n              precision    recall  f1-score   support\n\n           1       0.53      0.40      0.46        58\n           2       0.39      0.36      0.38        55\n           3       0.31      0.24      0.27        63\n           4       0.35      0.48      0.41        58\n           5       0.43      0.53      0.47        49\n\n    accuracy                           0.40       283\n   macro avg       0.40      0.40      0.40       283\nweighted avg       0.40      0.40      0.39       283",
    "crumbs": [
      "04.- MP3",
      "mp3.html"
    ]
  },
  {
    "objectID": "03.- API Extractor/reddit.html",
    "href": "03.- API Extractor/reddit.html",
    "title": "Reddit API",
    "section": "",
    "text": "Este código utiliza la API de Reddit a través de la biblioteca praw para extraer datos del subreddit 'learnpython'. PRAW, acrónimo de “Python Reddit API Wrapper”, es un paquete de Python que permite un acceso sencillo a la API de Reddit.\nPara configurar las credenciales de autenticación de Reddit, debes crear un archivo de configuración llamado praw.ini. Este archivo debe estar ubicado en el mismo directorio que tu script.\nEl archivo praw.ini debe tener el siguiente formato:\n[CREDENCIALES]\nclient_id=YOUR_CLIENT_ID\nclient_secret=YOUR_CLIENT_SECRET\npassword=YOUR_REDDIT_PASSWORD\nusername=YOUR_REDDIT_USERNAME\nuser_agent=YOUR_USER_AGENT\nAsegúrate de reemplazar YOUR_CLIENT_ID, YOUR_CLIENT_SECRET, YOUR_PASSWORD, YOUR_USERNAME y YOUR_USER_AGENT con tus propias credenciales de Reddit.\nUna vez que hayas creado y guardado este archivo, PRAW lo utilizará automáticamente para autenticar tus solicitudes a la API de Reddit.\n\nimport praw\n\nreddit = praw.Reddit(\"CREDENCIALES\")\nsubreddit = reddit.subreddit('lies')\n\nLuego, obtiene los 500,000 posts principales del subreddit y almacena información relevante de cada post (título, puntuación, id, subreddit, url, número de comentarios, texto del post y fecha de creación) en una lista.\n\nposts = []\nfor post in subreddit.new(limit=3000):\n    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created, post.author, post.upvote_ratio, post.num_reports, post.mod_reports])\n\nFinalmente, convierte esta lista en un DataFrame de pandas y muestra las primeras 10 filas del DataFrame.\n\nimport pandas as pd\n\ndf = pd.DataFrame(posts, columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created','author','upvote_ratio', 'num_reports','mod_reports'])\n\ndf.to_csv('reddit_v4.csv', index=False)\n\ndf['created'] = pd.to_datetime(df['created'], unit='s')  # Se convierte la fecha a un formato legible\n\ndf.head(10)\n\n\n\n\n\n\n\n\ntitle\nscore\nid\nsubreddit\nurl\nnum_comments\nbody\ncreated\nauthor\nupvote_ratio\nnum_reports\nmod_reports\n\n\n\n\n0\nThis was the map of Europe in 1919\n1\n1g6kyyo\nlies\nhttps://i.redd.it/f3sxeknxajvd1.jpeg\n0\n\n2024-10-18 15:35:59\nTheSip69\n1.00\nNone\n[]\n\n\n1\nThe lineup of the New Fantastic 4 is really co...\n1\n1g6kxyf\nlies\nhttps://i.redd.it/kz0pbeqrajvd1.jpeg\n0\n\n2024-10-18 15:35:04\nAskJeevesIsBest\n1.00\nNone\n[]\n\n\n2\nThis is how America looks like after WW3\n8\n1g6j3mf\nlies\nhttps://i.redd.it/2lqn5bqnwivd1.jpeg\n2\n\n2024-10-18 14:15:58\nMilesAhXD\n0.79\nNone\n[]\n\n\n3\nI’m playing Valorant\n2\n1g6j15h\nlies\nhttps://i.redd.it/1odykr14wivd1.jpeg\n0\n\n2024-10-18 14:12:56\nFar_Departure_1580\n1.00\nNone\n[]\n\n\n4\nCamels and giraffes are the only animals\n9\n1g6it4r\nlies\nhttps://i.redd.it/yjjmtz4auivd1.png\n0\n\n2024-10-18 14:02:52\nAvailable-Zombie1208\n1.00\nNone\n[]\n\n\n5\nif you see evil ye YOU WILL DIE‼️!!!!!1!\n36\n1g6iqa3\nlies\nhttps://i.redd.it/lppnelnqtivd1.png\n10\n\n2024-10-18 13:59:37\nrealweekdays133\n0.98\nNone\n[]\n\n\n6\nThis image sucks\n223\n1g6idkr\nlies\nhttps://i.redd.it/uk85n4stqivd1.jpeg\n9\n\n2024-10-18 13:43:16\nKiraethu\n0.99\nNone\n[]\n\n\n7\nWhy did Mr breast do this?\n134\n1g6ht7q\nlies\nhttps://i.redd.it/eecokhzzlivd1.jpeg\n5\n\n2024-10-18 13:16:15\nCertifiedboykisser2\n0.99\nNone\n[]\n\n\n8\nI was banned from /r/lies for a post I'm going...\n1\n1g6dk96\nlies\nhttps://i.redd.it/ab12kz3tahvd1.png\n0\n\n2024-10-18 08:53:14\nImmaRussian\n1.00\nNone\n[]\n\n\n9\nI'm fluent in spanish\n3\n1g6gqpf\nlies\nhttps://www.reddit.com/r/lies/comments/1g6gqpf...\n1\n\n2024-10-18 12:22:24\nbigbig-dan\n1.00\nNone\n[]\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n\n\n# Calculate the number of posts by each author\nauthor_counts = df['author'].value_counts().reset_index()\nauthor_counts.columns = ['author', 'post_count']\n\n# Display the DataFrame\nprint(author_counts)\n\n# Plot the relationship between authors and their post counts\nplt.figure(figsize=(12, 6))\nsns.barplot(x='author', y='post_count', data=author_counts)\nplt.title('Number of Posts by Each Author')\nplt.xlabel('Author')\nplt.ylabel('Number of Posts')\nplt.xticks(rotation=90)  # Rotate author names for better readability\nplt.show()\n\n                   author  post_count\n0       Organic-Smell4743          16\n1     Nervous-Estate-1852          11\n2     SavageFractalGarden           9\n3                   d_-_p           8\n4            Niteshade_YT           8\n..                    ...         ...\n689              Preeeeow           1\n690             comfybuck           1\n691              20195780           1\n692          SpookyWeebou           1\n693  Intelligent-Fuel1485           1\n\n[694 rows x 2 columns]\n\n\n\n\n\n\n\n\n\ntipos de analisis\n\nprimer analisis temporal\n\n\npandas\nnumpy\nmatplotlib\nseaborn\n\n\nagrupar para hacer los graficos\ndistribuciones, boxplot, etc XDXD\n\nsi no existen datos temporales no se pueden analizar\ntres tipos de analisis minimo\nla primera tarea es hacer un analisis exploratorio de dato (EDA en ingles)\nen otro marco, analisis temporal, analisis de agrupacion, analisis de distribucion,\nya se puede hacer un analisis de distribucion por autor\nratio: It’s the ratio between upvotes and total votes. For example, if a post has 3 upvotes and 1 downvote, it has 75% upvote rate because 3 is 75% of 4.\n\nimport matplotlib.pyplot as plt \n\ndata = pd.read_csv('reddit_v2.csv')\n\nfig, a = plt.subplots(1, 2, figsize = (18, 5))\n\na[0].scatter(data['upvote_ratio'], data['num_comments'], color='blue')\n\n\nplt.show()",
    "crumbs": [
      "03.- API Extractor",
      "Reddit API"
    ]
  }
]